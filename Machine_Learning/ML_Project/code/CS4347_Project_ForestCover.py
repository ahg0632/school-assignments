# -*- coding: utf-8 -*-
"""CS4347_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oUQQiAomkY7Cy3NRcrONiL8wxGulX3xz
"""



"""# Project: Forest Cover Type Prediction

## Introduction

We want to identify the type of trees that are covering an area based on the image
provided. Furthermore, the data provided contains forests that have very minimal
human intervention and disturbances. Identifying these fields will help identify other fields without
needing to invest a lot of time in a survey team.

##  Requirements
"""

!pip install xgboost

import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from xgboost import XGBClassifier

"""## Data Preprocessing

The dataset is tabular and the dataset will be cleaned to fit a `Neural Netowork` and a `Random Forest Tree`
"""

# Load the datasets
train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')

# Print the head of the training dataset
print("Training dataset:")
print(train_df.head())

# Print the head of the testing dataset
print("\nTesting dataset:")
print(test_df.head())

# Check for null values in the training dataset
print("\nNull values in training dataset:")
print(train_df.isnull().sum())

# Check for null values in the testing dataset
print("\nNull values in testing dataset:")
print(test_df.isnull().sum())

# Remove 'Id' column from training set
train_df = train_df.drop('Id', axis=1)

# Remove 'Id' column from testing set
test_df = test_df.drop('Id', axis=1)

# Create copies for Neural Network and Random Forest
train_nn = train_df.copy()
test_nn = test_df.copy()
train_rf = train_df.copy()
test_rf = test_df.copy()


# Columns to normalize (0-1 scaling)
cols_to_normalize_01 = ['Elevation', 'Aspect', 'Horizontal_Distance_To_Hydrology',
                        'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',
                        'Horizontal_Distance_To_Fire_Points']

# Columns to normalize (0-255 scaling)
cols_to_normalize_0255 = ['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']


def normalize_columns(df, cols, min_val, max_val):
    scaler = MinMaxScaler(feature_range=(min_val, max_val))
    df[cols] = scaler.fit_transform(df[cols])
    return df


# Normalize for Neural Network
train_nn = normalize_columns(train_nn, cols_to_normalize_01, 0, 1)
test_nn = normalize_columns(test_nn, cols_to_normalize_01, 0, 1)

train_nn = normalize_columns(train_nn, cols_to_normalize_0255, 0, 255)
test_nn = normalize_columns(test_nn, cols_to_normalize_0255, 0, 255)


# Normalize for Random Forest
train_rf = normalize_columns(train_rf, cols_to_normalize_01, 0, 1)
test_rf = normalize_columns(test_rf, cols_to_normalize_01, 0, 1)

train_rf = normalize_columns(train_rf, cols_to_normalize_0255, 0, 255)
test_rf = normalize_columns(test_rf, cols_to_normalize_0255, 0, 255)


print(train_nn.head())
print(test_nn.head())

print("Size of Random Forest training set:", train_rf.shape)
print("Size of Neural Network training set:", train_nn.shape)
print("Size of Random Forest testing set:", test_rf.shape)
print("Size of Neural Network testing set:", test_nn.shape)

"""## Methodology & Results

### Neural Network Model
"""

# Extract features (X) and target (y)
X = train_nn.drop('Cover_Type', axis=1).values
y = train_nn['Cover_Type'].values -1 # Adjust labels to be 0-6

# Split data into training and validation sets
X_train_nn, X_val_nn, y_train_nn, y_val_nn = train_test_split(X, y, test_size=0.2, random_state=42)

# Regularization
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6
)

# Define the model
model_nn = tf.keras.models.Sequential([
    tf.keras.Input(shape=(X_train_nn.shape[1],)),
    tf.keras.layers.Dense(256, activation='sigmoid'),
    tf.keras.layers.Dense(128, activation='sigmoid'),
    tf.keras.layers.Dense(64, activation='sigmoid'),
    tf.keras.layers.Dense(32, activation='sigmoid'),
    tf.keras.layers.Dense(16, activation='sigmoid'),
    tf.keras.layers.Dense(7, activation='linear')  # Output layer with 7 units (1-7)
])

# Summarize the model
model_nn.summary()

# Compile the model
model_nn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# Train the model
history_nn = model_nn.fit(X_train_nn, y_train_nn, epochs=100, validation_data=(X_val_nn, y_val_nn), callbacks=[reduce_lr])

# Make predictions on the validation set
predictions_nn = model_nn.predict(X_val_nn)

# Apply softmax
predicted_probabilities_nn = tf.nn.softmax(predictions_nn).numpy()

# Get predicted classes
predicted_classes_nn = np.argmax(predicted_probabilities_nn, axis=1)

# Plot the loss curve
plt.figure(figsize=(10, 5))
plt.plot(history_nn.history['loss'], label='Training Loss')
plt.plot(history_nn.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Loss Curve')
plt.show()

# Plot the accuracy curve
plt.figure(figsize=(10, 5))
plt.plot(history_nn.history['accuracy'], label='Training Accuracy')
plt.plot(history_nn.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Accuracy Curve')
plt.show()

# Confusion Matrix
cm = confusion_matrix(y_val_nn, predicted_classes_nn)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

# Classification Report
print(classification_report(y_val_nn, predicted_classes_nn, zero_division=0))

"""### Random Forest Model"""

# Extract features (X) and target (y)
X_rf = train_rf.drop('Cover_Type', axis=1).values
y_rf = train_rf['Cover_Type'].values

# Split data into training and validation sets
X_train_rf, X_val_rf, y_train_rf, y_val_rf = train_test_split(X_rf, y_rf, test_size=0.2, random_state=42)

# Sample split list
samples_split_list = [2, 10, 30, 50, 100, 200, 400]

# Max depth list
max_depth_list = [1, 2, 3, 4, 8, 16, 32, 64, 128]
n_estimators_list = [10, 50, 100, 300, 500]

train_acc_samples = []
val_acc_samples = []

for min_samples_split in samples_split_list:
    rf_model = RandomForestClassifier(min_samples_split=min_samples_split, random_state=60)
    rf_model.fit(X_train_rf, y_train_rf)
    y_train_pred = rf_model.predict(X_train_rf)
    y_val_pred = rf_model.predict(X_val_rf)
    train_acc_samples.append(accuracy_score(y_train_rf, y_train_pred))
    val_acc_samples.append(accuracy_score(y_val_rf, y_val_pred))

plt.figure(figsize=(10, 5))
plt.plot(samples_split_list, train_acc_samples, marker='o')
plt.plot(samples_split_list, val_acc_samples, marker='o')
plt.title('Accuracy vs min_samples_split')
plt.xlabel('min_samples_split')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Validation'])
plt.grid(True)
plt.show()

# --- Random Forest with varying max_depth ---
train_acc_depth = []
val_acc_depth = []

for max_depth in max_depth_list:
    rf_model = RandomForestClassifier(max_depth=max_depth, random_state=60)
    rf_model.fit(X_train_rf, y_train_rf)
    y_train_pred = rf_model.predict(X_train_rf)
    y_val_pred = rf_model.predict(X_val_rf)
    train_acc_depth.append(accuracy_score(y_train_rf, y_train_pred))
    val_acc_depth.append(accuracy_score(y_val_rf, y_val_pred))

# Replace None with a label
x_labels = [str(d) if d is not None else "None" for d in max_depth_list]

plt.figure(figsize=(10, 5))
plt.plot(x_labels, train_acc_depth, marker='o')
plt.plot(x_labels, val_acc_depth, marker='o')
plt.title('Accuracy vs max_depth')
plt.xlabel('max_depth')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Validation'])
plt.grid(True)
plt.show()

train_acc_estimators = []
val_acc_estimators = []

for n in n_estimators_list:
    rf_model = RandomForestClassifier(n_estimators=n, random_state=60)
    rf_model.fit(X_train_rf, y_train_rf)
    y_train_pred = rf_model.predict(X_train_rf)
    y_val_pred = rf_model.predict(X_val_rf)
    train_acc_estimators.append(accuracy_score(y_train_rf, y_train_pred))
    val_acc_estimators.append(accuracy_score(y_val_rf, y_val_pred))

plt.figure(figsize=(10, 5))
plt.plot(n_estimators_list, train_acc_estimators, marker='o')
plt.plot(n_estimators_list, val_acc_estimators, marker='o')
plt.title('Accuracy vs n_estimators')
plt.xlabel('n_estimators')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Validation'])
plt.grid(True)
plt.show()

# Create and train the Random Forest model
rf_model = RandomForestClassifier(min_samples_split=30, max_depth=32, n_estimators=300, random_state=42)
rf_model.fit(X_train_rf, y_train_rf)

# Make predictions
y_train_pred_rf = rf_model.predict(X_train_rf)
y_pred_rf = rf_model.predict(X_val_rf)

# Evaluate the model
train_accuracy = accuracy_score(y_train_rf, y_train_pred_rf)
val_accuracy = accuracy_score(y_val_rf, y_pred_rf)
print(f"Train Accuracy: {train_accuracy}")
print(f"Validation Accuracy: {val_accuracy}")

# Generate the classification report
class_report = classification_report(y_val_rf, y_pred_rf)
print("Classification Report:\n", class_report)

# Generate and plot the confusion matrix
cm = confusion_matrix(y_val_rf, y_pred_rf)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

"""####  XGBoost"""

# Extract features (X) and target (y)
X_xgb = train_rf.drop('Cover_Type', axis=1).values
y_xgb = train_rf['Cover_Type'].values - 1 # Adjust labels to be 0-6

# Split data into training and validation sets
X_train_xgb, X_val_xgb, y_train_xgb, y_val_xgb = train_test_split(X_xgb, y_xgb, test_size=0.2, random_state=42)

# Initialize XGBoost classifier
xgb_model = XGBClassifier(objective='multi:softmax', num_class=7, random_state=42, early_stopping_rounds=16)

# Train the model with early stopping
eval_set = [(X_val_xgb, y_val_xgb)]
xgb_model.fit(X_train_xgb, y_train_xgb, eval_set=eval_set, verbose=True)

# Print the best iteration
print(f"Best iteration: {xgb_model.best_iteration}")

# Make predictions
y_pred_xgb = xgb_model.predict(X_val_xgb)

# Evaluate the model
accuracy = accuracy_score(y_val_xgb, y_pred_xgb)
print(f"Accuracy: {accuracy}")

# Generate the classification report
class_report = classification_report(y_val_xgb, y_pred_xgb)
print("Classification Report:\n", class_report)

# Generate and plot the confusion matrix
cm = confusion_matrix(y_val_xgb, y_pred_xgb)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()